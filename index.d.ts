/* auto-generated by NAPI-RS */
/* eslint-disable */
/** OxideEngine - Main struct for GPU-accelerated ML operations using Candle */
export declare class OxideEngine {
  /** Create a new OxideEngine instance with Metal backend (macOS) or CPU fallback */
  constructor()
  /** Get device information as a string */
  getDeviceInfo(): string
  /** Load a Phi-3 model from a safetensors file with tokenizer */
  loadModel(modelPath: string, tokenizerPath: string, configPath?: string | undefined | null): string
  /** Generate text from a prompt */
  generateText(prompt: string, maxTokens: number, options?: GenOptions | undefined | null): string
  /** Reset KV cache - call this when starting a new conversation */
  resetCache(): string
  /** Get current cache statistics */
  getCacheInfo(): CacheInfo
  /**
   * Run forward pass on the model with KV caching
   * This now only processes NEW tokens efficiently
   */
  forward(tokenIds: Array<number>): ForwardResult
  /**
   * Test GPU compute capability by performing a simple tensor operation
   * Creates two tensors on the GPU, adds them, and returns the result
   */
  testGpuCompute(): string
}

/** Cache information */
export interface CacheInfo {
  sequenceLength: number
  isEmpty: boolean
  message: string
}

/** Result of a forward pass */
export interface ForwardResult {
  batchSize: number
  sequenceLength: number
  vocabSize: number
  cacheLength: number
  topTokens: Array<TokenProb>
  message: string
}

/** Generation options for text generation */
export interface GenOptions {
  temperature?: number
  topP?: number
  seed?: number
  eosTokenId?: number
}

/** Token with its probability/logit */
export interface TokenProb {
  tokenId: number
  logit: number
}
